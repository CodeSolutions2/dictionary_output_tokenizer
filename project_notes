// Index

Tokenizer








// Try 0: Works!
		// outp.innerHTML += 'rowdata: ' + rowdata + "<br/>";
		// outp.innerHTML += 'index: ' + index + "<br/>";

		// Initially determined the name of the dictionary keys and values - valueNames: xs,ys
		// if (index == 0) { const valueNames = Object.keys(rowdata); outp.innerHTML += 'valueNames: ' + valueNames + "<br/>";}

		// X = tf.tensor(Object.values(rowdata.xs));
		// Y = tf.tensor(Object.values(rowdata.ys));

		// OR
		
		// The value of X and Y does not change per rowdata
		// X = await Object.values(rowdata.xs).toString();
		// Y = await Object.values(rowdata.ys).toString();

		 // Convert string tensor to javascript tensor
		 // X = X.toString();
		 // Y = Y.toString();


  // -------------------------------------------------
// How to read rows from a csvDataset
  // -------------------------------------------------

// Way 0 
// This is an object, and can iterate over values in the Object
const csv_iterator = await csvDataset.iterator();
	  
	  const tensors = await csvDataset.toArray();
tensors.forEach(async function(rowdata, index) {

	await csv_iterator.next().then(row => {

	      // row is [object Object] for Try 1 and 2
	     // outp.innerHTML += 'row: ' + row + "<br/>";
		  
	});  // end of await csv_iterator.next().then(row =>

});  // end of forEach


// -------------------------------------------------


// Way 1
// Same output as csvDataset, so just using csvDataset directly
// This is an object, and can iterate over values in the Object
const flattenedDataset = await csvDataset.map(({xs, ys}) => { return {xs:Object.values(xs), ys:Object.values(ys)}; });
// outp.innerHTML += 'flattenedDataset: ' +  flattenedDataset + "<br/>";
const csv_iterator_values = await flattenedDataset.iterator()
   
// This is an object, and can iterate over values in the Object
const csv_iterator = await csvDataset.iterator();
	  
const tensors = await csvDataset.toArray();
tensors.forEach(async function(rowdata, index) {

await csv_iterator_values.next().then(row => {
		
	      // row is [object Object] for Try 1 and 2
	     // outp.innerHTML += 'row: ' + row + "<br/>";
		  
	});  // end of await csv_iterator.next().then(row =>

});  // end of forEach


// -------------------------------------------------

# ---------------------------------------------
# Set up dataset in STORAGE
# ---------------------------------------------
PROJECT_ID="text-pr0cessing"
# ---------------------------------------------
gcloud projects create $PROJECT_ID
# ---------------------------------------------
gcloud config set project $PROJECT_ID
# ---------------------------------------------
# Enable project billing
# Obtain ACCOUNT_ID
gcloud alpha billing accounts list

ACCOUNT_ID=""

# Enable project billing
gcloud alpha billing accounts projects link $PROJECT_ID --billing-account=$ACCOUNT_ID

# View details about billing account
gcloud beta billing accounts describe $ACCOUNT_ID

# ---------------------------------------------

# Create Storage Bucket
BUCKET_NAME="textclassification-w-labeled-data"

LOCATION="europe-west9"
gcloud storage buckets create gs://$BUCKET_NAME --project=$PROJECT_ID \
                                                --default-storage-class=STANDARD \
                                                --location=$LOCATION \
                                                --uniform-bucket-level-access


git clone https://github.com/CodeSolutions2/text_classification_w_labels.git

# Upload file to storage
gcloud storage cp train_dataset0.csv gs://$BUCKET_NAME

gcloud storage ls --recursive gs://$BUCKET_NAME/**

# Make all objects in a bucket publicly readable
gcloud storage buckets add-iam-policy-binding gs://$BUCKET_NAME --member=allUsers --role=roles/storage.objectViewer

# ---------------------------------------------

touch cors.json

echo '[{"origin": ["https://yourdomain.com"],"responseHeader": ["Content-Type"],"method": ["GET", "HEAD"],"maxAgeSeconds": 3600}]' > cors.json

# OR

%%bash --err null
cat > cors.json <<EOF
[
    {
      "origin": ["https://CodeSolutions2.github.io"],
      "responseHeader": ["Content-Type"],
      "method": ["GET", "HEAD"],
      "maxAgeSeconds": 3600
    }
]
EOF

# Set CORS settings
gsutil cors set cors.json gs://$BUCKET_NAME

# ---------------------------------------------

# View saved CORS settings
gsutil cors get gs://$BUCKET_NAME

# ---------------------------------------------


https://cloud.google.com/storage/docs/gsutil/commands/cors

https://stackoverflow.com/questions/66050356/unable-to-fix-cors-policy-on-google-cloud-storage?rq=3

https://stackoverflow.com/questions/51615809/tensorflow-model-server-access-control-allow-origin?rq=3

https://stackoverflow.com/questions/57118200/tensorflowjs-google-storage-cors-error-with-tf-loadlayersmodel







// ---------------------------------------------
// Tokenizer
// ---------------------------------------------
  
// Make an array with word length
let len_X_array = [];
uniqueArray.forEach(async function(word, ind) { len_X_array.push(word.length); });

console.log("len_X_array: " + len_X_array);

// ---------------------------------------------


// -----------
// Way 0 : words with the same length will be assigned the same index
// -----------
// Sort the attribute vector
// https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/sort
// let len_X_array_sort = len_X_array.sort();
// console.log('len_X_array_sort:' + len_X_array_sort);

// Put the attribute vector into a dictionary with a corresponding index
// const dictt = Object.fromEntries(len_X_array_sort.map((atr2sort, index) => [atr2sort, index]));

// Make an array of zeros
//  let X_array_sorted = [...Array(uniqueArray.length).keys()].map((x) => 0);
  
//  uniqueArray.forEach(async function(word, ind) { 

	// word is in position ind=0 of X_array
//         let key = word.length;  // the attribute to sort
       
       // this gives me the index that X_array[word] should have in the output list X_array_sorted
//        let ind_sorted = dictt[key];

       // Print first 10 to see if correct
//        if (ind < 10){ console.log('word:' + word); console.log('key:' + key); console.log('ind_sorted:' + ind_sorted); }
       
       // but, word should be in position ind_sorted
//        X_array_sorted[ind_sorted] = word;
//  });

//  console.log('tokenizer0_X_array_sorted:' + X_array_sorted);

// Write a dictionary, where word=keys are ordered from longest word to shortest word.
// Thus, the longest word has the smallest index, and the shortest word has the largest index.

// var tokenizer0 = Object.fromEntries(X_array_sorted.map((word, index) => [word, index]));
  
// ---------------------------------------------

// -----------
// Way 1
// -----------

async function create_tokenizer_array_w_dict_per_row(uniqueArray) {

	// ------------------------------------------

	// Print the first 5 rows of an array
	uniqueArray.forEach(async function(row, ind) {
		// Print the first 5 key and value pairs
		if (ind < 5){ console.log( 'uniqueArray['+ ind + ']: ' + row ); }
		
	});
	  
	// ------------------------------------------

	// [0] Make an array, where each row is a dictionary where the keys are called name and value; name=word and value=word.length
	var array_w_dictionary_rows = [];
	uniqueArray.forEach(async function(word, ind) { 
		array_w_dictionary_rows.push({ name: word, value: word.length });
	});

	// ------------------------------------------

	// Print the first 5 rows of the array, array_w_dictionary_rows
	console.log('array_w_dictionary_rows:');
	print_an_array_containing_a_dictionary(array_w_dictionary_rows);
	  
	// ------------------------------------------
	
	// [1] Sort an array, containing a dictionary per row, where all the rows are sorted by the dictionary key called value in ascending order.
	// Sorting array of objects: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/sort
	var ascending_sorted_array = array_w_dictionary_rows.sort((a, b) => a.value - b.value);
	
	// ------------------------------------------
	  
	// Print the first 5 rows of the array, ascending_sorted_array
	console.log('ascending_sorted_array:');
	print_an_array_containing_a_dictionary(ascending_sorted_array);

	// ------------------------------------------

	// [2] Make the array be sorted in descending order, where all the rows are sorted by the dictionary key called value in descending order.
	var descending_sorted_array_w_dictionary_per_row = ascending_sorted_array.reverse();
	  
	// ------------------------------------------
	  
	// Print the first 5 rows of the array, descending_sorted_array_w_dictionary_per_row
	console.log('descending_sorted_array_w_dictionary_per_row:');
	print_an_array_containing_a_dictionary(descending_sorted_array_w_dictionary_per_row);

	// ------------------------------------------

	// [3] Reassign the number in the dictionary key called value, to a increasing count, per row of the descending sorted array
	var out_final = [];
	descending_sorted_array_w_dictionary_per_row.forEach(async function(row, ind) {

		let key = Object.values(row.name).toString();   // This prints the dictionary key per row 
		// let value = Object.values(row.value).toString(); // This prints the dictionary value per row

		// ---------------------
		
		// Try 0
		// descending_sorted_array_w_dictionary_per_row[ind] = {name: key, value: ind}; // This assigns name and value to the key using the number ind

		// Try 1
		// Want to reassign the dictionary value, using the key, per row
		// out_final[key] = ind;

		// Try 2
		
		out_final.push({name: key, value: ind})
	});

	// ------------------------------------------

	// Print the first 5 rows of the array, out_final
	console.log('out_final:');
	print_an_array_containing_a_dictionary(out_final);

	// ------------------------------------------

	  return out_final;
  }

	
  // -------------------------------------------------

	
  async function print_an_array_containing_a_dictionary(array) {
	  
	// Print the first 5 rows of an array, where each row contains a dictionary
	array.forEach(async function(row, ind) {
		
		// Print the available keys of the dictionary
		let list_the_keys_of_the_dict = Object.keys(row).toString();
		console.log( 'list_the_keys_of_the_dict: ' + list_the_keys_of_the_dict );
		// This returns: name,value

		// It is CORRECT sort of, but it prints commas in between each letter
		let key = Object.values(row.name);   // It should print the dictionary key per array row
		// OR
		let key1 = array[ind].toString();  // It prints [Object] object

		// It is INCORRECT, it prints nothing
		let value = Object.values(row.value); // It should print the dictionary value per array row

		// Print the first 5 key and value pairs
		if (ind < 5){ console.log( 'key: ' + key + ', key1: ' + key1 + ', value:' + value ); }
		
	});

  }

// ---------------------------------------------


    // Way 0: load pre-trained model
    // const MODEL_URL = 'model.json';
    // const custom_model = await tf.loadLayersModel(MODEL_URL);

    // OR

    // Way 1: load model layers using sequential
    
		// let model = tf.sequential();
		// model.add( tf.layers.embedding(inputDim=NUM_WORDS, outputDim=EMBEDDING_DIM, inputLength=MAXLEN) );
		// model.add( tf.layers.globalAveragePooling1d() );
		// model.add( tf.layers.dense(NUM_OF_CLASSES, activation='softmax') );

		// OR

	 // Way 2: load model layers using sequential
		let model = tf.sequential({layers: [tf.layers.embedding(inputDim=NUM_WORDS, outputDim=EMBEDDING_DIM, inputLength=MAXLEN), tf.layers.globalAveragePooling1d(), tf.layers.dense(NUM_OF_CLASSES, activation='softmax')]});
	 
		// Way 3: load model layers using functional API
		// const input = tf.input({shape: [5]});
	 // let Layer0 = tf.layers.embedding(inputDim=NUM_WORDS, outputDim=EMBEDDING_DIM, inputLength=MAXLEN);
	 // let Layer1 = tf.layers.globalAveragePooling1d();
	 // let Layer2 = tf.layers.dense(NUM_OF_CLASSES, activation='softmax');
		// const output = Layer2.apply(Layer1.apply(Layer0.apply(input)));
	 // const model = tf.model({inputs: input, outputs: output});
	
		// ---------------------------

		model.compile({optimizer: 'adam', loss: 'sparse_categorical_crossentropy', metrics: ["accuracy"]});
		

// ---------------------------------------------


	  
    // Number of columns
    // const num_of_cols = (await csvDataset.columnNames()).length;
    // outp.innerHTML += 'num_of_cols: ' + num_of_cols + "<br/>";

    // const column_names = (await csvDataset.columnNames());
    // outp.innerHTML += 'column_names: ' + column_names + "<br/>";

// ---------------------------------------------

  // Create a csvDataset using ONLY the correctly tokenized rows
	  // put x_clean and ys arrays in to a list of dictionaries per row with the same keys
	  // data should look like : [{a: 1, b: 11}, {a: 2, b: 12}, {a: 3, b: 13}, ...]
	  let data = []; 
	  correctly_tokenized_rows.forEach(async function(correct_ind, ind) { 

		let xs_seq_correct_row = xs_seq[correct_ind];
		  
		  
		// Post pad the sequences such that each sequence is [1, maxlen_local], or xs_seq is size [num_of_rows, maxlen_local]
		let amount2pad = maxlen_local - xs_seq_correct_row.length; // OK
		let array_of_zeros = Array.from({ length: amount2pad }, (_, i) => 0);
		let xs_seq_pad = xs_seq_correct_row.concat(array_of_zeros);

		// Does not print for some reason
		// if (ind == 0){
		// 	console.log("maxlen_local: " + maxlen_local);
		// 	console.log("xs_seq_correct_row.length: " + xs_seq_correct_row.length);
		// 	console.log("amount2pad: " + amount2pad);
			
		// 	console.log("xs_seq_correct_row: " + xs_seq_correct_row);
		// 	console.log("array_of_zeros: " + array_of_zeros);
		// 	console.log("xs_seq_pad: " + concatenated_array1);
		// }
		  
		// data.push({xs: xs_seq_pad, ys: ys[correct_ind]}); 
		data.push({xs: xs_seq[correct_ind], ys: ys[correct_ind]}); 
	  });
	
	  const  = await tf.data.array(data).batch(1);  // Output:  Returns: tf.data.Dataset 


// ---------------------------------------------


// ---------------------------------------------


// ---------------------------------------------


// ---------------------------------------------

// ---------------------------------------------

// ---------------------------------------------


// ---------------------------------------------

// ---------------------------------------------


// ---------------------------------------------





Ideas

0. use local Storage to save maxlen

1. add and Event listener for the change of value for maxlen, then update global variable

2. There is not guarantee that the 2 forEach statements will run in sequenqtial order - so add async/await to these two

// ---------------------------------------------


Design0 : this has too many sublevels I think
async train_model
	- await get_csvDataset ---> csvDataset
	- await modify_X_Y ---> csvDataset_sequence
		- await find_class_names ---> class_names (updates global variable)
		- tensors.forEach ---> xs, ys (NO await used)
		- await tokenize_X(xs, ys) 
			- xs_rest.forEach --> final_str (NO await used)
			- tokenizer = await create_tokenizer_dict(uniqueArray)
			- vocab_size = await update_global_variable_vocab_size(tokenizer) (updates global variable)
			- csvDataset_sequence = await apply_tokenizer_to_sentences(xs, ys, tokenizer)
				- xs.forEach --> xs_seq, maxlen_local, correctly_tokenized_rows (NO await used)
				- correctly_tokenized_rows.forEach --> csvDataset_sequence (NO await used)
	- await csvDataset_sequence.map
	- await synchronize_model_parameters

		

const tensors = await csvDataset.toArray();

tensors.forEach(async function(rowdata, index) {

	X = Object.values(rowdata.xs);
	Y = Object.values(rowdata.ys);
	console.log("X: " + X);
	console.log("Y: " + Y);

});  // end of forEach


// ---------------------------------------------


<!DOCTYPE html>
<html>
<head></head>
<body>

<!-- Text classification webapp -->
<!-- https://js.tensorflow.org/api/1.0.0/ -->
<h1 style='text-align: center; margin-bottom: -35px;'>Text classification webapp</h1>
<br><br>

<!-- [Step 0] Train model -->
<label for="train_dataset_url_label" style="display:block">Enter train dataset location url - url can be Github or GCP storage: (ie: https://github.com/CodeSolutions2/text_classification_w_labels/train_dataset0.csv)</label>
<input type="text" value="" placeholder="Train dataset url" id="train_dataset_url" rows="1" cols="500" size="200" style="display:block">
<br><br>
<label for="y_datasetLabels_label" style="display:block">Enter the y dataset labels: (ie: sport,business,politics,entertainment,tech)</label>
<input type="text" value="" placeholder="y dataset labels" id="y_datasetLabels" rows="1" cols="500" size="200" style="display:block">
<br>
<button id="train_model" onclick="train_model()" style="display:block">train_model</button>

	
<!-- [Step 1] Result: say if model is trained or not -->
<div id="output" style="font-family:courier;font-size:24px;height:300px"></div>

	
<!-- [Step 2] Predict/inference: predict each sentence separated by a \n character -->
<label for="input_text_label" style="display:none">Enter text to label, each sentence separated by the \n character will be labeled:</label>
<input type="text" value="" placeholder="Enter text to label" id="input_text" rows="10" cols="500" style="display:none">



	
<!-- https://developer.mozilla.org/en-US/docs/Web/CSS/position -->
<style>
canvas {border: 1px solid black; position: absolute; display: inline-block; z-index: 1; top: 150px;},
div {position: relative; z-index: 2;},
table {border-collapse: collapse;}
td,
th {border: 1px solid black; padding: 10px 20px;}
</style>

<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"></script>
<!-- <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-node@4.17.0/dist/index.min.js"></script> -->

<script>

  // -------------------------------------------------
	
  const outp = document.getElementById('output');

  var csvDataset = [];  // This is a global variable

  var vocab_size = 0;  // This is a global variable, that will be reassigned after button push

  var maxlen = 0;  // This is a global variable, that will be reassigned after button push
	
  var class_names = []; // This is a global variable, that will be reassigned after button push




  // -------------------------------------------------

  async function get_csvDataset(csvDataset) {

	// https://github.com/CodeSolutions2/text_classification_w_labels/train_dataset0.csv
	// https://storage.googleapis.com/textclassification-w-labeled-data/train_dataset0.csv
	  
	const datasetUrl = document.getElementById("train_dataset_url").value;

	let out = datasetUrl.split("/");
	// console.log("out:" + out);
	
	let domain_name = out[2];
	// console.log("domain_name:" + domain_name);

	  var url_vec = [];
	
	if (domain_name == 'github.com'){
		const repoOwner = out[3];
		const repoName = out[4];
		
		var url = `https://api.github.com/repos/${repoOwner}/${repoName}/contents`;
		
		    await fetch(url).then(res => res.json()).then(data => {
		    data.forEach(file => {
		      if (file.type === 'file' && file.name.match(/.(csv|txt)$/i)) {
			url_vec.push(file.download_url);
		      }
		    });
		  }).catch(error => { outp.innerHTML += error; });

		console.log("url_vec:" + url_vec);
		
    // Select the first dataset from the url list
    const csvUrl = url_vec[0];
    console.log("csvUrl: " + csvUrl);

    // Make a Dataset  
    // X and y (dataset object)
    csvDataset = await tf.data.csv(csvUrl, { columnConfigs: { Y: {isLabel: true} } });
    console.log("csvDataset: " + csvDataset);  // csvDataset: [object Object]
		

	} else if (domain_name == 'storage.googleapis.com') {

	   //    const BUCKET_NAME = out[3];
	//  const fileName = out[4];
		// var url = `https://storage.googleapis.com//${BUCKET_NAME}/${fileName}`;

		// var options = {method : 'get', headers: {'Content-Type': "application/json", 'Access-Control-Allow-Origin': '*'}, mode: 'cors'};
		
		// await fetch(url, options).then(res => res.json()).then(data => {
		//     data.forEach(file => {
		 //      if (file.type === 'file' && file.name.(/.(csv|txt)$/i)) {
			// url_vec1.push(file.download_url);
		 //      }
		  //   });
		//   }).catch(error => { outp.innerHTML += error; });
		
		// OR
		
		// url_vec.push(datasetUrl);

		// OR

		 // Create <form> element to submit parameters to endpoint.
    //     var form = document.createElement('form');

       // form.setAttribute('method', 'GET'); 
     //   var url = 'https://storage.googleapis.com/textclassification-w-labeled-data';
   //      form.setAttribute('action', url);

        // Parameters to pass to endpoint.
  //       var params = {'key': 'train_dataset0.csv'};

        // Add form parameters as hidden input values.
    //     for (var p in params) {
     //      var input = document.createElement('input');
     //      input.innerHTML = '<input style="display:none;" name=' +p+ 'value=' +params[p]+ '>';

		// https://storage.googleapis.com/textclassification-w-labeled-data?keytype%3D%22text%22=train_dataset0.csv
		// https://storage.googleapis.com/textclassification-w-labeled-data?keyvalue%3Dtrain_dataset0.csv=
          
 //          form.appendChild(input);
 //        }
        // Add form to page and submit it to open the endpoint.
  //       document.body.appendChild(form);

   //      form.submit();
		
    //     url_vec.push( url + '/' + params['key'] );

        // Cross-Origin Request Blocked: The Same Origin Policy disallows reading the remote resource at https://storage.googleapis.com/textclassification-w-labeled-data/train_dataset0.csv. (Reason: CORS header ‘Access-Control-Allow-Origin’ missing). Status code: 200

		// console.log("url_vec:" + url_vec);

		// OR

		const BUCKET_NAME = out[3];
		const fileName = out[4];
		var url = `https://storage.googleapis.com//${BUCKET_NAME}/${fileName}`;
		var options = {method : 'get', headers: {'Access-Control-Allow-Origin': '*'}, mode: 'no-cors'};

		csvDataset = await fetch(url, options).then(res => res).then(res => {   
		 const out = tf.data.csv(res.text(), { columnConfigs: { Y: { isLabel: true } } });
		 console.log("out: " + out.toString() );
		 return out; }).catch(error => { outp.innerHTML += error; });

		// OR

		// The key "Y" provided in columnConfigs does not  any of the column names (<!DOCTYPE html>).
		// const response = await fetch(url, { mode: 'no-cors' });
		// const data = await response.text();
		// csvDataset = tf.data.csv(data, { columnConfigs: { Y: { isLabel: true } } });
		
	} else {
		outp.innerHTML = 'Please enter a GitHub repository or Google Cloud Platform Storage URL';
	}
 
	return csvDataset;
  }

	
  // -------------------------------------------------


async function find_class_names(ylabel_str) {
	
	let ylabel_vec = ylabel_str.split(",");

	// y label should be unique, but find unique values just in case
	class_names = [...new Set(ylabel_vec)];

	 return class_names;
 }

	
  // -------------------------------------------------


async function print_a_csvDataset(csvDataset){
	
	const tensors = await csvDataset.toArray();
	tensors.forEach(async function(rowdata, index) {

		if (index < 5){
			
			X = Object.values(rowdata.xs);
			Y = Object.values(rowdata.ys);
			console.log("X: " + X);
			console.log("Y: " + Y);
		}
	
	});  // end of forEach

}


  // -------------------------------------------------

	
  async function train_model() {
	  
    // ---------------------------

    // [Step 0] Read dataset from location to tensorflow csvDataset object
    csvDataset = await get_csvDataset();  // Update global variable

    // ---------------------------

    // [Step 3] Clean the dataset
    await modify_X_Y(csvDataset).then(async function(csvDataset_sequence) {

	    // ---------------------------
	    
	    // Print final data to see if it is correct
	    await print_a_csvDataset(csvDataset_sequence);
	    
	    // ---------------------------
		    
	    // Convert the xs and ys to a flattenedDataset
	    const flattenedDataset = await csvDataset_sequence.map(({xs, ys}) => { return {xs:Object.values(xs), ys:Object.values(ys)}; });

	    // ---------------------------

	    // Load the model parameters
	    // out = [NUM_WORDS, EMBEDDING_DIM, MAXLEN, NUM_OF_CLASSES]
	    let out = await synchronize_model_parameters();
	    console.log("out: " + out);

	    // ---------------------------

	    // Load the model
	    //let model = await tf.sequential({layers: [tf.layers.embedding(inputDim=out[0], outputDim=out[1], inputLength=out[2]), tf.layers.globalAveragePooling1d(), tf.layers.dense(out[3], activation='softmax')]});

	  // await model.compile({optimizer: 'adam', loss: 'sparse_categorical_crossentropy', metrics: ["accuracy"]});

	    // ---------------------------

	    // Train the model
	   // await model.fitDataset(flattenedDataset, { epochs: 10, callbacks: { onEpochEnd: async (epoch, logs) => { console.log(epoch + ':' + logs.loss); } }  });

	    // ---------------------------
	
	    // Save model to localStorage
	  //  await model.save('localstorage://my-model-1');

	    // ---------------------------
	    
    });  // end of modify_X_Y(csvDataset)
	  
    // ---------------------------


  }  // end of train_model

	
  // -------------------------------------------------

 
 async function synchronize_model_parameters() {
	 
	  let NUM_WORDS = await Number(vocab_size);
	  console.log("NUM_WORDS: " + NUM_WORDS);  // NUM_WORDS: 1241
	 
	  let EMBEDDING_DIM = 256;
	 
	  //let MAXLEN = await Number(maxlen); // the global variable was too slow at updating
	  console.log("maxlen: " + maxlen);
	  // OR
	  let MAXLEN = localStorage.getItem("maxlen_local");
	 localStorage.removeItem("maxlen_local");
	  console.log("MAXLEN: " + MAXLEN);
	 
	  
	  let NUM_OF_CLASSES = await class_names.length;
	  console.log("NUM_OF_CLASSES: " + NUM_OF_CLASSES);  // NUM_OF_CLASSES: 5
	 
	return [NUM_WORDS, EMBEDDING_DIM, MAXLEN, NUM_OF_CLASSES];
 }

	
  // -------------------------------------------------


  async function modify_X_Y(csvDataset) {

	  const tensors = await csvDataset.toArray();

	  // tensors.length: 10
	  // outp.innerHTML += 'num_of_rows = tensors.length: ' + tensors.length + "<br/>";  

	  // tensors: [object Object],[object Object],[object Object],[object Object],[object Object],[object Object],[object Object],[object Object],[object Object],[object Object]
	  // outp.innerHTML += 'tensors: ' + tensors + "<br/>";

	  // ---------------------------
	  // Handling desired y labels
	  // ---------------------------
	  // Obtain desired labels from user
	  var ylabel_str = document.getElementById("y_datasetLabels").value;
	  
	  // Convert string tensor to javascript tensor, Make text lower case		  
	  ylabel_str = ylabel_str.toString().toLowerCase();
	  
	  var class_names = await find_class_names(ylabel_str);
	  console.log("class_names: " + class_names);
	  
	  // Tokenize Y using user input labels
	  const y_assignment = Object.fromEntries(class_names.map((key, index) => [key, index]));
	  // ---------------------------
	  
          // ---------------------------
          // Read data from csv object, clean each row, validate if X and Y are correct, tokenize X, 
	  // save tokenized X and Y to a flattenedDataset
	  // ---------------------------
	  var X = '';
	  var Y = '';
	  var xs = [];
	  var ys = [];

	  undesireable_words = ["about", "above", "after", "again", "against", "all", "and", "any", "are", "because", "been", "before", "being", "below", "between", "both", "but", "could", "did", "does", "doing", "down", "during", "each", "few", "for", "from", "further", "had", "has", "have", "having", "he'd", "he'll", "he's", "her", "here", "here's", "hers", "herself", "him", "himself", "his", "how", "how's", "i", "i'd", "i'll", "i'm", "i've", "into", "is", "it", "it's", "its", "itself", "let's", "me", "more", "most", "myself", "nor", "of", "on", "once", "only", "or", "other", "ought", "our", "ours", "ourselves", "out", "over", "own", "same", "she", "she'd", "she'll", "she's", "should", "so", "some", "such", "than", "that", "that's", "the", "their", "theirs", "them", "themselves", "then", "there", "there's", "these", "they", "they'd", "they'll", "they're", "they've", "this", "those", "through", "to", "too", "under", "until", "up", "very", "was", "we", "we'd", "we'll", "we're", "we've", "were", "what", "what's", "when", "when's", "where", "where's", "which", "while", "who", "who's", "whom", "why", "why's", "with", "would", "you", "you'd", "you'll", "you're", "you've", "your", "yours", "yourself", "yourselves", '</p>', '<a', 'id=', "href=", 'title=', 'class=', '</a>', '</sup>', '<p>', '</b>', '<sup'];

	  tensors.forEach(async function(rowdata, index) {
		
		 // ---------------------------
		 // Clean text procedure
		 // ---------------------------
		 // Make text lower case, Remove characters between parenthesis, Remove text that are 1 or 2 characters long, Remove undesireable characters
		 X = Object.values(rowdata.xs).toString().toLowerCase().replace(/\((.*?)\)/g, '').replace(/\{(.*?)\}/g, '').replace(/\[(.*?)\]/g, '').replace(/[\.\€\$\£\%\d,\[\]\(\)\{\}\!-><\n]/g, '').replace(/(?<=\s)[A-Za-z]{1,2}(?=\s)/g, '');  
		Y = Object.values(rowdata.ys).toString().toLowerCase();
		  
		 // Remove undesireable words 
		 // Maybe memory intensive - **** run after everything works ****
		 // undesireable_words.forEach(function(word2remove, index) {X = X.replace(/\b${word2remove}\b/g, '');}
		  // ---------------------------

		  // ---------------------------
                  // Using user input labels to validate X and Y values 
		  // ---------------------------
		  regexp = new RegExp(`${Y}`, 'g');
		  iterator = ylabel_str.matchAll(regexp);
		  let matches7 = Array.from(iterator); // Convert iterator to array
		  // outp.innerHTML += 'matches7: ' + matches7 + "<br/>";  // matches7: business
		  // outp.innerHTML += 'matches7.length: ' + matches7.length + "<br/>";  // matches7.length: 1

		  // ---------------------------
		  // Verify that data is correct, and only keep correct X and Y data
		  if (X.length-1 > 10 && matches7.length == 1){
		       // Save non-tokenized X values and tokenized Y values to array
		       xs.push(X);
		       ys.push(y_assignment[Y]);
		  }

	  });  // end of forEach

	// ---------------------------
	  
	  // Call X Tokenizer function
	  let csvDataset_sequence = await tokenize_X(xs, ys);
	  
	  // ---------------------------	
	  
	  return csvDataset_sequence;
  }


  // -------------------------------------------------

	
  async function print_a_dictionary(dictt) {
	  
	// Print the first XX keys of a dictionary
	var dictt_keys = Object.keys(dictt);

	for (var i=0; i<dictt_keys.length; i++) {
		let key = dictt_keys[i];
		let value = dictt[key];
		console.log('key: ' + key +  ', value:' +  value);
	}

  }

	
  // -------------------------------------------------

async function sort_an_array_by_string_attribute(str_arr){

	let arr = str_arr; // initialize final variable
	c = 0;
	while (c != arr.length-1) {
		 c = 0;
		  for (var i=0; i<arr.length-1; i++) {
			let cur = arr[i];
			  let next = arr[i+1];
			  let cur_value = cur.length;  // attribute that string array is sorted by
			  let next_value = next.length; // attribute that string array is sorted by
			  
			  if (cur_value < next_value) {
				  temp_cur = arr[i]
				  temp_next = arr[i+1]
				  arr[i+1] = temp_cur
				  arr[i] = temp_next
			  } else {
				  //this will be some value from 1 to arr.length-1
				  c = c + 1;
				  // c = arr.length-1 means that sorting is finished
			  }
		  }
	}
	
	return arr;
}

  // -------------------------------------------------
	
  async function create_tokenizer_dict(uniqueArray) {

	// ------------------------------------------

	// [0] Sort uniqueArray by an attribute (ie: length)
	let arr = await sort_an_array_by_string_attribute(uniqueArray);
	  
	// ------------------------------------------
  
	// [1] Make a dictionary where each key is a word in arr, and each value is index
	// Create a dictionary from one array
	let tokenizer = Object.fromEntries(arr.map((word, index) => [word, index]));

	// ------------------------------------------

	// [2] Print the first 5 rows of the array, array_w_dictionary_rows
	// console.log('print_a_dictionary:');
	// print_a_dictionary(tokenizer);

	// ------------------------------------------

	 return tokenizer;
  }
	
	
  // -------------------------------------------------


async function tokenize_X(xs, ys) {
	
	// ---------------------------
	
	// Concatenate the array contents into a long text string
	
	// Initialize with first array
	let final_str = xs.slice(0,1);
	// console.log('final_str 1st part:' + final_str);

	let xs_rest = xs.slice(1, xs.length); // 9 [Correct]
	// console.log('xs_rest:' + xs_rest.length);
	
	xs_rest.forEach(async function(val, ind) { 
		final_str = final_str.concat(val); // string
		// console.log('index: ' + ind + ', final_str type :' + typeof final_str); 
	});

	final_str = final_str.toString();
	// console.log('final_str accumulated type :' + typeof final_str); // final_str accumulated type :string [Correct]
	
	// ---------------------------
	
	// Replace any multiple spaces from accumulatted string with only a single space
	final_str = final_str.replace(/\s+/g, ' ');
	// console.log('final_str accumulated :' + final_str);
	
	// ---------------------------

	// Split the accumulatted string
	let final_arr = final_str.split(" ");
	// console.log('final_arr.length :' + final_arr.length); // final_arr.length :2807

	// ---------------------------

	// Find unique words
	let uniqueArray = [...new Set(final_arr)];

	// Add the <OOV> token: if the word is not in the token list assign this value
	uniqueArray.push('<OOV>');
	// console.log('uniqueArray.length :' + uniqueArray.length); // uniqueArray.length :1241

	// ---------------------------
	
	let tokenizer = await create_tokenizer_dict(uniqueArray);
	
	// ---------------------------
	
	// Update global variable, one has to return the variable from a function to make global variables update. One can not just call "vocab_size = Object.keys(tokenizer).length;" because this call means that the variable is local to this function.
	vocab_size = await update_global_variable_vocab_size(tokenizer);
	// console.log("vocab_size: " + vocab_size);

	// ---------------------------

	// Apply tokenizer to sentences
	// Return both variables as an array: out = [xs_seq, maxlen, correctly_tokenized_rows]
	let csvDataset_sequence = await apply_tokenizer_to_sentences(xs, ys, tokenizer);

	// ---------------------------
	
	return csvDataset_sequence;
}


  // -------------------------------------------------

	
  async function update_global_variable_vocab_size(tokenizer) {
	return Object.keys(tokenizer).length;
  }

  // -------------------------------------------------

	
  async function update_global_variable_maxlen(maxlen_local) {
	  
	// Way 0: save variable in localStorage
	localStorage.setItem("maxlen_local", maxlen_local);

	// Way 1: return variable so that the global variable is updated
	return maxlen_local;
  }

// -------------------------------------------------

	
async function apply_tokenizer_to_sentences(xs, ys, tokenizer) {

	// Apply tokenizer to sentences, to obtain sequences
	let maxlen_local = 0;
	let xs_seq = [];
	let correctly_tokenized_rows = [];
	
	xs.forEach(async function(val, ind) {
		// console.log("val original: " + val);
		
		// Replace any multiple spaces with only a single space
		val = val.replace(/\s+/g, ' ');
		
		// Convert string to string_array
		let sen_arr = val.split(" ");

		// Sort the string_array by word length
		sen_arr = await sort_an_array_by_string_attribute(sen_arr);
		// console.log("sen_arr: " + sen_arr);
		
		// Obtain the length of array for model training variable MAXLEN
		if (maxlen_local < sen_arr.length) { maxlen_local = sen_arr.length;}

		sen_arr.forEach(async function(val1, ind1) {
			
			// Directly call the tokenizer, if the word does not exist put the number for '<OOV>' token
			let result = tokenizer[val1] ?? tokenizer['<OOV>'];
			
			// Replace all [val1 occurences] in the [string val], with the [token value result]
			regexp = new RegExp(`${val1}`, 'g');

			// Update the sentence string
			val = val.replace(regexp, result.toString());
			
		});  // end of sen_arr.forEach

		// Convert string to string_array
		let seq_arr = val.split(" ");

		// Convert string_array to integer array
		let seq_arr_int = [];
		seq_arr.forEach(async function(val, ind) { seq_arr_int.push(Number(val)); });
		// console.log("seq_arr_int: " + seq_arr_int);

		// Check for incorrect replacement due to browser speed
		if (Math.max(seq_arr_int) <= (vocab_size-1)) {
			// Save the row number that was correctly tokenized
			correctly_tokenized_rows.push(ind);
		}
				
		// Save output sequence per row, replacement is correct
		xs_seq.push(seq_arr_int);
		
	});  // end of forEach

	// Outputs: xs_seq, maxlen_local, correctly_tokenized_rows

	// ---------------------------

	// Update global variable, one can return the variable from a function to make global variables update faster. So by calling it in a separate function return the variable updates faster than it would if one called "maxlen = maxlen_local" because it would update after apply_tokenizer_to_sentences returned, not in the middle of apply_tokenizer_to_sentences.
	maxlen = await update_global_variable_maxlen(maxlen_local);
	
	// ---------------------------

  // Create a csvDataset using ONLY the correctly tokenized rows
	  // put x_clean and ys arrays in to a list of dictionaries per row with the same keys
	  // data should look like : [{a: 1, b: 11}, {a: 2, b: 12}, {a: 3, b: 13}, ...]
	  let data = []; 
	  correctly_tokenized_rows.forEach(async function(correct_ind, ind) { 
		let xs_seq_correct_row = xs_seq[correct_ind];
		  
		// Post pad the sequences such that each sequence is [1, maxlen_local], or xs_seq is size [num_of_rows, maxlen_local]
		let amount2pad = maxlen_local - xs_seq_correct_row.length; // OK
		let array_of_zeros = Array.from({ length: amount2pad }, (_, i) => 0);
		let xs_seq_pad = xs_seq_correct_row.concat(array_of_zeros);
		  
		// data.push({xs: xs_seq_pad, ys: ys[correct_ind]}); 
		data.push({xs: xs_seq[correct_ind], ys: ys[correct_ind]}); 
	  });
	
	// ---------------------------
	
	return tf.data.array(data).batch(1);  // Output:  Returns: tf.data.Dataset 
}
	
  // -------------------------------------------------

	
  async function predict_with_custom_model(y_assignment) {

	  const loadedModel = await tf.loadLayersModel('localstorage://my-model-1');
    
    var pred_vec = [['', '', ''],['row', 'prediction', 'probability']]; 

    const text_input = document.getElementById("input_text").value;

    text_input = text_input.split("\n")

    // ---------------------------
	  
    // Tokenize X
    // sequence_array
	  
    // ---------------------------

    var pred_vec = [];
		  
    sequence_array.forEach(async function(seq, ind) {
	  
	    // Call the custom_model 
	    const result = loadedModel.predict(seq) ;

	    // Get index of maximum softmax probability 
	    const index = result.as1D().argMax().dataSync()[0];
			
	    // Get maximum softmax probability 
	    const resultData = await result.data();    
	    const maxprob = resultData[index];

	    pred_vec.push([y_assignment[index], maxprob]);    
    
    });  // end of forEach

    // ---------------------------
	  
    // Save array to csv and put download csv file link on screen (NO AUTOMATIC DOWNLOADING)
    await put_results_in_CSVfile(pred_vec);

    // Create a table dynamically with the results
    await generateTable_dynamically(pred_vec);

    // ---------------------------	

  }  // end of predict_with_custom_model

	
  // -------------------------------------------------

	
  async function put_results_in_CSVfile(pred_vec) {
    
    // ---------------------
    // Puts array into csv format
    const blob = new Blob([pred_vec], { type: 'text/csv;charset=utf-8;' });

    // Create a url for the data object
    const url = URL.createObjectURL(blob);
    // ---------------------
    // OR
    // ---------------------
    // Puts array into csv format
    // let csvContent = "data:text/csv; charset=utf-8";
    // pred_vec.forEach(function(row_array) {
	// csvContent += row_array.join(",") + "\r\n";
    // });
    // OR
    // let csvContent = "data:text/csv;charset=utf-8," + rows.map(e => e.join(",")).join("\n");
	  
    // Create a url for the data object
    // var url = encodeURI(csvContent);
    // ---------------------

    const link = document.createElement("a");
    link.setAttribute("href", url);

    const filename = 'data.csv';
    link.setAttribute("download", filename);
	  
    link.style.display = 'block';

    outp.innerHTML += 'Download CSV here: ' + url + "<br/>";
    // document.body.appendChild(link);

    // Automatic download of csv
    // link.click();
    // document.body.removeChild(link);
	  
  }

	
  // -------------------------------------------------

	
  async function generateTable_dynamically(pred_vec) {

  	const tbl = document.createElement("table");
  
  	const tblBody = document.createElement("tbody");
  
  	// Create row cells dynamically
  	for (let i=0; i < pred_vec.length; i++){
  		// create a table row
  		const row = document.createElement("tr");
  
  		for (let j=0; j < pred_vec[0].length; j++){
  			const cell = document.createElement("td");
  			const cellText = document.createTextNode(`${pred_vec[i][j]}`);
  			cell.appendChild(cellText);
  			row.appendChild(cell);
  		}
  
  		// add a row to the end of the table
  		tblBody.appendChild(row);
  	}
  	tbl.appendChild(tblBody);
  
  	document.body.appendChild(tbl);

  }  

  // -------------------------------------------------

	
</script>
</body>
</html>


