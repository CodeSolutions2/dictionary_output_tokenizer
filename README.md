# tokenizer

The purpose of this repository is to demonstrate/compare different ways to tokenize text, for text classification and/or generation.

[Current working versiion of word tokenization] https://codesolutions2.github.io/tokenizer/index2.html


## In progress
  - A more rigorous version of the word tokenizer
  - Subword tokenization
