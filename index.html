<!DOCTYPE html>
<html>
<head></head>
<body>

<!-- Text classification webapp -->
<!-- https://js.tensorflow.org/api/1.0.0/ -->
<h1 style='text-align: center; margin-bottom: -35px;'>Text classification webapp</h1>
<br><br>

<!-- [Step 0] Train model -->
<label for="train_dataset_url_label" style="display:block">Enter train dataset location url: url can be Github or GCP storage (ie: https://github.com/CodeSolutions2/text_classification_w_labels/train_dataset.csv):</label>
<input type="text" value="" placeholder="Train dataset url" id="train_dataset_url" rows="1" cols="500" style="display:block">
<br>
<button id="train_model" onclick="train_model()" style="display:block">train_model</button>

	
<!-- [Step 1] Result: say if model is trained or not -->
<div id="output" style="font-family:courier;font-size:24px;height:300px"></div>

	
<!-- [Step 2] Predict/inference: predict each sentence separated by a \n character -->
<label for="input_text_label" style="display:none">Enter text to label, each sentence separated by the \n character will be labeled:</label>
<input type="text" value="" placeholder="Enter text to label" id="input_text" rows="10" cols="500" style="display:none">



	
<!-- https://developer.mozilla.org/en-US/docs/Web/CSS/position -->
<style>
canvas {border: 1px solid black; position: absolute; display: inline-block; z-index: 1; top: 150px;},
div {position: relative; z-index: 2;},
table {border-collapse: collapse;}
td,
th {border: 1px solid black; padding: 10px 20px;}
</style>

<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"></script>

<script>

  // -------------------------------------------------
	
  const outp = document.getElementById('output');

  var url_vec = [];  // This is a global variable, and I start referencing it in get_train_dataset

	
  // -------------------------------------------------

	
  async function get_train_datasetURL() {

	// https://github.com/CodeSolutions2/text_classification_w_labels/train_dataset0.csv
	// https://storage.googleapis.com/BUCKET_NAME/train_dataset.csv
	const datasetUrl = document.getElementById("train_dataset_url").value;

	let out = datasetUrl.split("/")

	outp.innerHTML += "out: " + out + "<br/>";
	
	let domain_name = out[2];
	
	if (domain_name == 'github.com'){
		const repoOwner = out[3];
		const repoName = out[4];

		outp.innerHTML += "repoOwner: " + repoOwner + "<br/>";
		outp.innerHTML += "repoName: " + repoName + "<br/>";
		
		var url = `https://api.github.com/repos/${repoOwner}/${repoName}/contents`;
		    //var options = {method : 'get', headers: headers, mode: 'no-cors'};
		    await fetch(url).then(res => res.json()).then(data => {
		    data.forEach(file => {
		      if (file.type === 'file' && file.name.match(/.(csv|txt)$/i)) {
		        
		        //outp.innerHTML += "Filename=" + file.name + ", file url=" + file.download_url + "<br/>";
			url_vec.push(file.download_url);
		      }
		    });
		  }).catch(error => { outp.innerHTML += error; });

	} else if (domain_name == 'storage.googleapis.com') {
		url_vec.push(datasetUrl);
		
	} else {
		outp.innerHTML = 'Please enter a GitHub repository or Google Cloud Platform Storage URL';
	}
	
 
	return url_vec;
  }
	
	
  // -------------------------------------------------

	
  async function train_model() {

    // ---------------------------

    // [Step 0] Read in a list of urls that are .csv or .txt files
    
    var url_vec = await get_train_datasetURL();
    outp.innerHTML += 'url_vec: ' + url_vec + "<br/>";

    // ---------------------------
	  
    // [Step 1] Load custom model
	  
    // Way 0: load pre-trained model
    // const MODEL_URL = 'https://storage.googleapis.com/tensorflowjsmodels0/model.json';
    // const MODEL_URL = 'model.json';
    // const custom_model = await tf.loadLayersModel(MODEL_URL);

    // OR

    // Way 1: load model layers using sequential
    // const NUM_WORDS = ;
    // const EMBEDDING_DIM = 256;
    //  const MAXLEN 
    //  const num_of_classes
    // const custom_model = tf.sequential();
    // custom_model.add( tf.layers.embedding(inputDim=NUM_WORDS, outputDim=EMBEDDING_DIM, inputLength=MAXLEN) );
    // custom_model.add( tf.layers.globalAveragePooling1d() );
    // custom_model.add( tf.layers.dense(num_of_classes, activation='softmax') );

    // OR

    // Way 2: load model layers using functional API (tf.LayersModel)
	  
    // ---------------------------

    // custom_model.compile({optimizer='adam', loss='sparse_categorical_crossentropy', metrics=["accuracy"]});
	  
    // ---------------------------
	  
    // [Step 2] Select a url from the url list
	  
    // url_vec.forEach(function(csvUrl, index) {
    // Does not make sense to train the model for every dataset found.
    // It makes more sense to train the model on one seleccted dataset, then test it with predictions.

    const csvUrl = url_vec[0];

    // ---------------------------

    // [Step 3] Make a Dataset
	  
    // X and y (dataset object)
    const csvDataset = await tf.data.csv(csvUrl, { columnConfigs: { Y: {isLabel: true} } });
    outp.innerHTML += 'csvDataset: ' +  csvDataset + "<br/>";  // csvDataset: [object Object]
	    
    // ---------------------------
	  
    // Number of features is the number of column names minus one for the label column.
    const numOfFeatures = (await csvDataset.columnNames()).length - 1;
    outp.innerHTML += 'numOfFeatures: ' + numOfFeatures + "<br/>";
	  
    // ---------------------------
	  
	  
    // });  // end of forEach


    // ---------------------------

    // [Step 4] Clean the dataset
    await modify_X_Y(csvDataset);
	  
    // ---------------------------

    // [Step 5] Tokenize X
	  

    // ---------------------------

    // [Step 5] Tokenize Y

    // ---------------------------

    // [Step 5] Fit the model using the prepared Dataset
   // await custom_model.fitDataset(flattenedDataset, {
   //   epochs: 10,
   //   callbacks: {
   //     onEpochEnd: async (epoch, logs) => { console.log(epoch + ':' + logs.loss); }
    //  }
   // });

    // ---------------------------

    return	

  }  // end of train_model

	
  // -------------------------------------------------


	
  // -------------------------------------------------


  async function modify_X_Y(csvDataset) {

	  // Line 218 = TypeError: can't convert undefined to object
	  // const flattenedDataset = csvDataset.map(({X, Y}) => { return {X:Object.values(X), Y:Object.values(Y)};}).batch(1);
	  // outp.innerHTML += 'flattenedDataset: ' +  flattenedDataset + "<br/>";
	  // const it = await flattenedDataset.iterator()

	  // OR
	  
	  // TypeError: X.toLowerCase is not a function
	  const csv_iterator = await csvDataset.iterator();

	  // OR
	  
	  // No output 
	  // const datasetArray1 = await Array.from(csvDataset)
	  // outp.innerHTML += 'datasetArray1: ' + datasetArray1 + "<br/>";

	  // OR

	  const tensors = await csvDataset.toArray();
	  // var dataout = tf.tensor(tensors);
	  
	  outp.innerHTML += 'num_of_rows = tensors.length: ' + tensors.length + "<br/>";  // tensors.length: 10

	  // TypeError: tensors.data is not a function
	  // outp.innerHTML += 'num_of_cols = dataout.data()[0].length: ' + dataout.data()[0].length + "<br/>";
	  // outp.innerHTML += 'tensors[0][0]: ' + tensors[0][0] + "<br/>";  // tensors[0][0]: undefined
	  
	  // outp.innerHTML += 'dataout.data()[0][0]: ' + dataout.data()[0][0] + "<br/>";  // dataout[0][0]: undefined
	  
	  
	  // ---------------------------
	  
	   const xs = [];
	   const ys = [];
	  var X;
	  var Y;

	  tensors.forEach(async function(rowdata, index) {
	   // for (let i = 0; i < tensors.length; i++) {

		  // row.dataSync is not a function
	await csv_iterator.next().then(row => {tf.tensor(row.value.xs).dataSync()}).then(row => {
		  
	  //await csv_iterator.next().then(row => {
	      outp.innerHTML += 'row: ' + row + "<br/>";

	      // --------------------------------------
	      // Get values from the object row
	      // const get_rowvalues = row.dataSync();
		
	      // OR
		
	      //async (row) => {
		//      const X_tensor = await tf.tensor(row.value.xs).dataSync();
		//      outp.innerHTML += 'X_tensor: ' + X_tensor + "<br/>";
	     // };

		  
	      // --------------------------------------

	      // --------------------------------------
	      // Assign X and Y
		  
	      // X = await get_rowvalues.value.xs; 

	      // OR
		  
	      // X = row.value.xs;  // returns empty value
	     //  Y = row.value.ys;  // returns empty value
	     //  outp.innerHTML += 'X: ' + X + "<br/>";
	     //  outp.innerHTML += 'Y: ' + Y + "<br/>";

	      // valueNames: value,done
	      // if (index == 0) { const valueNames = Object.keys(row); outp.innerHTML += 'valueNames: ' + valueNames + "<br/>";}

              // valueNames: xs,ys
	      // if (index == 0) { const valueNames = Object.keys(row.value); outp.innerHTML += 'valueNames: ' + valueNames + "<br/>";}
	  });

	      // OR
		  
	      // let row = await csv_iterator.next();
	      // X = row.value.X.dataSync();
	      // Y = row.value.Y.dataSync();
	      // outp.innerHTML += 'X: ' + X + "<br/>";
	      // outp.innerHTML += 'Y: ' + Y + "<br/>";

	      // OR
		  
// TypeError: row.value is undefined
	      //await new Promise(row => {outp.innerHTML += 'row.value.xs: ' + row.value.xs + "<br/>";});

	      // OR
		  
	      // rowdata.value: undefined
	      // outp.innerHTML += 'rowdata.value: ' + rowdata.value + "<br/>";

	      // X = tensors.data()[i][0]
	      // Y = tensors.data()[i][1]
	      // outp.innerHTML += 'X: ' + X + "<br/>";

	      X = 'TEST';
		  
	      // Make text lower case
	      X = X.toLowerCase();
	
	      // Remove characters between parenthesis
	      //X.replace(/\((.*?)\)/g, '');
	      //X.replace(/\{(.*?)\}/g, '');
	      //X.replace(/\[(.*?)\]/g, '');
	
	      // [Step 3] Remove undesireable words 
	
	      // [Step 4] Remove sentences with less than 10 words. Narrow the sentences down to realistic sentences.
		   
	      xs.push(X)
	      ys.push(Y)

	  });

	  outp.innerHTML += 'xs: ' + xs + "<br/>";

	  outp.innerHTML += 'ys: ' + ys + "<br/>";

  }

	
  // -------------------------------------------------

	
  async function predict_with_custom_model(y_assignment) {
    
    var pred_vec = [['', '', ''],['row', 'prediction', 'probability']]; 

    const text_input = document.getElementById("input_text").value;

    text_input = text_input.split("\n")

    // ---------------------------
	  
    // Tokenize X
    // sequence_array
	  
    // ---------------------------

    var pred_vec = [];
		  
    sequence_array.forEach(async function(seq, ind) {
	  
	    // Call the custom_model 
	    const result = custom_model.predict(seq) ;

	    // Get index of maximum softmax probability 
	    const index = result.as1D().argMax().dataSync()[0];
			
	    // Get maximum softmax probability 
	    const resultData = await result.data();    
	    const maxprob = resultData[index];

	    pred_vec.push([y_assignment[index], maxprob]);    
    
    });  // end of forEach

    // ---------------------------
	  
    // Save array to csv and put download csv file link on screen (NO AUTOMATIC DOWNLOADING)
    await put_results_in_CSVfile(pred_vec);

    // Create a table dynamically with the results
    await generateTable_dynamically(pred_vec);

    // ---------------------------	

  }  // end of predict_with_custom_model

	
  // -------------------------------------------------

	
  async function put_results_in_CSVfile(pred_vec) {
    
    // ---------------------
    // Puts array into csv format
    const blob = new Blob([pred_vec], { type: 'text/csv;charset=utf-8;' });

    // Create a url for the data object
    const url = URL.createObjectURL(blob);
    // ---------------------
    // OR
    // ---------------------
    // Puts array into csv format
    // let csvContent = "data:text/csv; charset=utf-8";
    // pred_vec.forEach(function(row_array) {
	// csvContent += row_array.join(",") + "\r\n";
    // });
    // OR
    // let csvContent = "data:text/csv;charset=utf-8," + rows.map(e => e.join(",")).join("\n");
	  
    // Create a url for the data object
    // var url = encodeURI(csvContent);
    // ---------------------

    const link = document.createElement("a");
    link.setAttribute("href", url);

    const filename = 'data.csv';
    link.setAttribute("download", filename);
	  
    link.style.display = 'block';

    outp.innerHTML += 'Download CSV here: ' + url + "<br/>";
    // document.body.appendChild(link);

    // Automatic download of csv
    // link.click();
    // document.body.removeChild(link);
	  
  }

	
  // -------------------------------------------------

	
  async function generateTable_dynamically(pred_vec) {

  	const tbl = document.createElement("table");
  
  	const tblBody = document.createElement("tbody");
  
  	// Create row cells dynamically
  	for (let i=0; i < pred_vec.length; i++){
  		// create a table row
  		const row = document.createElement("tr");
  
  		for (let j=0; j < pred_vec[0].length; j++){
  			const cell = document.createElement("td");
  			const cellText = document.createTextNode(`${pred_vec[i][j]}`);
  			cell.appendChild(cellText);
  			row.appendChild(cell);
  		}
  
  		// add a row to the end of the table
  		tblBody.appendChild(row);
  	}
  	tbl.appendChild(tblBody);
  
  	document.body.appendChild(tbl);

  }  

  // -------------------------------------------------

	
</script>
</body>
</html>
