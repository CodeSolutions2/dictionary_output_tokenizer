<!DOCTYPE html>
<html>
<head></head>
<body>

<!-- Text classification webapp -->
<!-- https://js.tensorflow.org/api/1.0.0/ -->
<h1 style='text-align: center; margin-bottom: -35px;'>Text classification webapp</h1>
<br><br>

<!-- [Step 0] Train model -->
<label for="train_dataset_url_label" style="display:block">Enter train dataset location url - url can be Github or GCP storage: (ie: https://github.com/CodeSolutions2/text_classification_w_labels/train_dataset0.csv)</label>
<input type="text" value="" placeholder="Train dataset url" id="train_dataset_url" rows="1" cols="500" size="200" style="display:block">
<br><br>
<label for="y_datasetLabels_label" style="display:block">Enter the y dataset labels: (ie: sport,business,politics,entertainment,tech)</label>
<input type="text" value="" placeholder="y dataset labels" id="y_datasetLabels" rows="1" cols="500" size="200" style="display:block">
<br>
<button id="train_model" onclick="train_model()" style="display:block">train_model</button>

	
<!-- [Step 1] Result: say if model is trained or not -->
<div id="output" style="font-family:courier;font-size:24px;height:300px"></div>

	
<!-- [Step 2] Predict/inference: predict each sentence separated by a \n character -->
<label for="input_text_label" style="display:none">Enter text to label, each sentence separated by the \n character will be labeled:</label>
<input type="text" value="" placeholder="Enter text to label" id="input_text" rows="10" cols="500" style="display:none">



	
<!-- https://developer.mozilla.org/en-US/docs/Web/CSS/position -->
<style>
canvas {border: 1px solid black; position: absolute; display: inline-block; z-index: 1; top: 150px;},
div {position: relative; z-index: 2;},
table {border-collapse: collapse;}
td,
th {border: 1px solid black; padding: 10px 20px;}
</style>

<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"></script>
<!-- <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-node@4.17.0/dist/index.min.js"></script> -->

<script>

  // -------------------------------------------------
	
  const outp = document.getElementById('output');

  var csvDataset = [];  // This is a global variable

  var tokenizer = {};  // This is a global variable, that will be reassigned after button push
  var maxlen = 0;  // This is a global variable, that will be reassigned after button push
  var class_names = []; // This is a global variable, that will be reassigned after button push
  var custom_model = 0; 
	
  // -------------------------------------------------

	
  async function get_csvDataset(csvDataset) {

	// https://github.com/CodeSolutions2/text_classification_w_labels/train_dataset0.csv
	// https://storage.googleapis.com/textclassification-w-labeled-data/train_dataset0.csv
	  
	const datasetUrl = document.getElementById("train_dataset_url").value;

	let out = datasetUrl.split("/");
	// console.log("out:" + out);
	
	let domain_name = out[2];
	// console.log("domain_name:" + domain_name);

	  var url_vec = [];
	
	if (domain_name == 'github.com'){
		const repoOwner = out[3];
		const repoName = out[4];
		
		var url = `https://api.github.com/repos/${repoOwner}/${repoName}/contents`;
		
		    await fetch(url).then(res => res.json()).then(data => {
		    data.forEach(file => {
		      if (file.type === 'file' && file.name.match(/.(csv|txt)$/i)) {
			url_vec.push(file.download_url);
		      }
		    });
		  }).catch(error => { outp.innerHTML += error; });

		console.log("url_vec:" + url_vec);
		
    // Select the first dataset from the url list
    const csvUrl = url_vec[0];
    console.log("csvUrl: " + csvUrl);

    // Make a Dataset  
    // X and y (dataset object)
    csvDataset = await tf.data.csv(csvUrl, { columnConfigs: { Y: {isLabel: true} } });
    console.log("csvDataset: " + csvDataset);  // csvDataset: [object Object]
		

	} else if (domain_name == 'storage.googleapis.com') {

	   //    const BUCKET_NAME = out[3];
	//  const fileName = out[4];
		// var url = `https://storage.googleapis.com//${BUCKET_NAME}/${fileName}`;

		// var options = {method : 'get', headers: {'Content-Type': "application/json", 'Access-Control-Allow-Origin': '*'}, mode: 'cors'};
		
		// await fetch(url, options).then(res => res.json()).then(data => {
		//     data.forEach(file => {
		 //      if (file.type === 'file' && file.name.(/.(csv|txt)$/i)) {
			// url_vec1.push(file.download_url);
		 //      }
		  //   });
		//   }).catch(error => { outp.innerHTML += error; });
		
		// OR
		
		// url_vec.push(datasetUrl);

		// OR

		 // Create <form> element to submit parameters to endpoint.
    //     var form = document.createElement('form');

       // form.setAttribute('method', 'GET'); 
     //   var url = 'https://storage.googleapis.com/textclassification-w-labeled-data';
   //      form.setAttribute('action', url);

        // Parameters to pass to endpoint.
  //       var params = {'key': 'train_dataset0.csv'};

        // Add form parameters as hidden input values.
    //     for (var p in params) {
     //      var input = document.createElement('input');
     //      input.innerHTML = '<input style="display:none;" name=' +p+ 'value=' +params[p]+ '>';

		// https://storage.googleapis.com/textclassification-w-labeled-data?keytype%3D%22text%22=train_dataset0.csv
		// https://storage.googleapis.com/textclassification-w-labeled-data?keyvalue%3Dtrain_dataset0.csv=
          
 //          form.appendChild(input);
 //        }
        // Add form to page and submit it to open the endpoint.
  //       document.body.appendChild(form);

   //      form.submit();
		
    //     url_vec.push( url + '/' + params['key'] );

        // Cross-Origin Request Blocked: The Same Origin Policy disallows reading the remote resource at https://storage.googleapis.com/textclassification-w-labeled-data/train_dataset0.csv. (Reason: CORS header ‘Access-Control-Allow-Origin’ missing). Status code: 200

		// console.log("url_vec:" + url_vec);


		// OR

		const BUCKET_NAME = out[3];
		const fileName = out[4];
		var url = `https://storage.googleapis.com//${BUCKET_NAME}/${fileName}`;
		var options = {method : 'get', headers: {'Access-Control-Allow-Origin': '*'}, mode: 'no-cors'};

		csvDataset = await fetch(url, options).then(res => res).then(res => {   
		 const out = tf.data.csv(res.text(), { columnConfigs: { Y: { isLabel: true } } });
		 console.log("out: " + out.toString() );
		 return out; }).catch(error => { outp.innerHTML += error; });

		// OR

		// The key "Y" provided in columnConfigs does not  any of the column names (<!DOCTYPE html>).
		// const response = await fetch(url, { mode: 'no-cors' });
		// const data = await response.text();
		// csvDataset = tf.data.csv(data, { columnConfigs: { Y: { isLabel: true } } });
		
	} else {
		outp.innerHTML = 'Please enter a GitHub repository or Google Cloud Platform Storage URL';
	}
 
	return csvDataset;
  }

	
  // -------------------------------------------------


async function find_class_names(ylabel_str) {
	
	let ylabel_vec = ylabel_str.split(",");

	// y label should be unique, but find unique values just in case
	class_names = [...new Set(ylabel_vec)];

	 return class_names;
 }

	
  // -------------------------------------------------

	
  async function train_model() {
	  
    // ---------------------------

    // [Step 0] Read dataset from location to tensorflow csvDataset object
    csvDataset = await get_csvDataset();  // Update global variable

    // ---------------------------
	  
    // Number of columns
    // const num_of_cols = (await csvDataset.columnNames()).length;
    // outp.innerHTML += 'num_of_cols: ' + num_of_cols + "<br/>";

    // const column_names = (await csvDataset.columnNames());
    // outp.innerHTML += 'column_names: ' + column_names + "<br/>";

    // ---------------------------

    // [Step 3] Clean the dataset
    await modify_X_Y(csvDataset).then(async function(csvDataset_sequence) {

	    // ---------------------------
		    
	    // Convert the xs and ys to a flattenedDataset
	    const flattenedDataset = await csvDataset_sequence.map(({xs, ys}) => { return {xs:Object.values(xs), ys:Object.values(ys)}; });

	    // ---------------------------

	    // Load the model
	    custom_model = await make_and_compile_custom_model();
	    console.log("custom_model: " + custom_model);
	    
	    // ---------------------------
	    
	    // Fit the model using the prepared Dataset
	    await custom_model.fitDataset(flattenedDataset, {
		    epochs: 10,
		    callbacks: {
			    onEpochEnd: async (epoch, logs) => { console.log(epoch + ':' + logs.loss); }
		    }
	    });

	    // ---------------------------

	    // Save the model if I can
	    const saveResults = await custom_model.save('localstorage://my-model-1');
	    
	    
    });  // end of modify_X_Y(csvDataset)
	  
    // ---------------------------


  }  // end of train_model

	
  // -------------------------------------------------

 

	
 async function make_and_compile_custom_model() {
	
    // [Step 1] Load custom model
	  
    // Way 0: load pre-trained model
    // const MODEL_URL = 'https://storage.googleapis.com/tensorflowjsmodels0/model.json';
    // const MODEL_URL = 'model.json';
    // const custom_model = await tf.loadLayersModel(MODEL_URL);

    // OR

    // Way 1: load model layers using sequential
    const NUM_WORDS = tokenizer.length;
    console.log("NUM_WORDS: " + NUM_WORDS);
	 
    const EMBEDDING_DIM = 256;
	 
    const MAXLEN = maxlen;
    console.log("MAXLEN: " + MAXLEN);
	  
    const NUM_OF_CLASSES = class_names.length;
    console.log("NUM_OF_CLASSES: " + NUM_OF_CLASSES);
	 
    const custom_model = tf.sequential();
    custom_model.add( tf.layers.embedding(inputDim=NUM_WORDS, outputDim=EMBEDDING_DIM, inputLength=MAXLEN) );
    custom_model.add( tf.layers.globalAveragePooling1d() );
    custom_model.add( tf.layers.dense(NUM_OF_CLASSES, activation='softmax') );

    // OR

    // Way 2: load model layers using functional API (tf.LayersModel)
	console.log('here');
	  
    // ---------------------------

    custom_model.compile({optimizer: 'adam', loss: 'sparse_categorical_crossentropy', metrics: ["accuracy"]});
	 
	return custom_model;
 }

	
  // -------------------------------------------------


  async function modify_X_Y(csvDataset) {

	  // ---------------------------
	  // Module 0: input=csvDataset, output=xs, ys
	  // ---------------------------
	  const tensors = await csvDataset.toArray();

	  // tensors.length: 10
	  // outp.innerHTML += 'num_of_rows = tensors.length: ' + tensors.length + "<br/>";  

	  // tensors: [object Object],[object Object],[object Object],[object Object],[object Object],[object Object],[object Object],[object Object],[object Object],[object Object]
	  // outp.innerHTML += 'tensors: ' + tensors + "<br/>";

	  // ---------------------------
	  // Handling desired y labels
	  // ---------------------------
	  // Obtain desired labels from user
	  var ylabel_str = document.getElementById("y_datasetLabels").value;
	  
	  // Convert string tensor to javascript tensor, Make text lower case		  
	  ylabel_str = ylabel_str.toString().toLowerCase();
	  
	  var class_names = await find_class_names(ylabel_str);
	  console.log("class_names: " + class_names);
	  
	  // Tokenize Y using user input labels
	  const y_assignment = Object.fromEntries(class_names.map((key, index) => [key, index]));
	  
	  // outp.innerHTML += 'y_assignment["business"]: ' + y_assignment['business'] + "<br/>";  // correct
	  // outp.innerHTML += 'y_assignment.keys(): ' + y_assignment.keys() + "<br/>";
	  // outp.innerHTML += 'y_assignment.values(): ' + y_assignment.values() + "<br/>";
	  // ---------------------------
	  
          // ---------------------------
          // Read data from csv object, clean each row, validate if X and Y are correct, tokenize X, 
	  // save tokenized X and Y to a flattenedDataset
	  // ---------------------------
	  var X = '';
	  var Y = '';
	  var xs = [];
	  var ys = [];

	  undesireable_words = ["about", "above", "after", "again", "against", "all", "and", "any", "are", "because", "been", "before", "being", "below", "between", "both", "but", "could", "did", "does", "doing", "down", "during", "each", "few", "for", "from", "further", "had", "has", "have", "having", "he'd", "he'll", "he's", "her", "here", "here's", "hers", "herself", "him", "himself", "his", "how", "how's", "i", "i'd", "i'll", "i'm", "i've", "into", "is", "it", "it's", "its", "itself", "let's", "me", "more", "most", "myself", "nor", "of", "on", "once", "only", "or", "other", "ought", "our", "ours", "ourselves", "out", "over", "own", "same", "she", "she'd", "she'll", "she's", "should", "so", "some", "such", "than", "that", "that's", "the", "their", "theirs", "them", "themselves", "then", "there", "there's", "these", "they", "they'd", "they'll", "they're", "they've", "this", "those", "through", "to", "too", "under", "until", "up", "very", "was", "we", "we'd", "we'll", "we're", "we've", "were", "what", "what's", "when", "when's", "where", "where's", "which", "while", "who", "who's", "whom", "why", "why's", "with", "would", "you", "you'd", "you'll", "you're", "you've", "your", "yours", "yourself", "yourselves", '</p>', '<a', 'id=', "href=", 'title=', 'class=', '</a>', '</sup>', '<p>', '</b>', '<sup'];

	  tensors.forEach(async function(rowdata, index) {
		
		 // ---------------------------
		 // Clean text procedure
		 // ---------------------------
		 // Make text lower case, Remove characters between parenthesis, Remove text that are 1 or 2 characters long, Remove undesireable characters
		 X = Object.values(rowdata.xs).toString().toLowerCase().replace(/\((.*?)\)/g, '').replace(/\{(.*?)\}/g, '').replace(/\[(.*?)\]/g, '').replace(/[\.\€\$\£\%\d,\[\]\(\)\{\}\!-><\n]/g, '').replace(/(?<=\s)[A-Za-z]{1,2}(?=\s)/g, '');  
		Y = Object.values(rowdata.ys).toString().toLowerCase();
		  
		 // Remove undesireable words 
		 // Maybe memory intensive - **** run after everything works ****
		 // undesireable_words.forEach(function(word2remove, index) {X = X.replace(/\b${word2remove}\b/g, '');}
		  // ---------------------------

		  // ---------------------------
                  // Using user input labels to validate X and Y values 
		  // ---------------------------
		  regexp = new RegExp(`${Y}`, 'g');
		  iterator = ylabel_str.matchAll(regexp);
		  let matches7 = Array.from(iterator); // Convert iterator to array
		  // outp.innerHTML += 'matches7: ' + matches7 + "<br/>";  // matches7: business
		  // outp.innerHTML += 'matches7.length: ' + matches7.length + "<br/>";  // matches7.length: 1

		  // ---------------------------
		  // Verify that data is correct, and only keep correct X and Y data
		  if (X.length-1 > 10 && matches7.length == 1){
		       // outp.innerHTML += 'X: ' + X + "<br/>";
		       // outp.innerHTML += 'Y: ' + Y + "<br/>";

		       // Save non-tokenized X values and tokenized Y values to array
		       xs.push(X);
		       ys.push(y_assignment[Y]);
		  }

	  });  // end of forEach
	  // ---------------------------
	  //  Module 0 END : input=csvDataset, output=xs, ys
	  // ---------------------------


	// ---------------------------
	  
	  // Call X Tokenizer function
	  var xs_seq = await tokenize_X(xs);

	// ---------------------------
	  
	  // Create a csvDataset
	  // put x_clean and ys arrays in to a list of dictionaries per row with the same keys
	  // data should look like : [{a: 1, b: 11}, {a: 2, b: 12}, {a: 3, b: 13}, ...]
	  let data = [];
	  xs_seq.forEach(async function(xs_val, ind) {
		data.push({xs: xs_val, ys: ys[ind]});
	  });  // end of forEach
	  const Dataset_sequence = tf.data.array(data).batch(1);  // Output:  Returns: tf.data.Dataset 
	  
	  // print dataset
	  // await Dataset_sequence.forEachAsync(e => { console.log('{'); for(var key in e) { console.log(key+':'); e[key].print(); } console.log('}'); });
	  
	  
	  return Dataset_sequence
  }


  // -------------------------------------------------

	
  async function print_a_dictionary(dictt) {
	  
	// Print the first XX keys of a dictionary
	var dictt_keys = Object.keys(dictt);

	for (var i=0; i<dictt_keys.length; i++) {
		let key = dictt_keys[i];
		let value = dictt[key];
		console.log('key: ' + key +  ', value:' +  value);
	}

  }

	
  // -------------------------------------------------

async function sort_an_array_by_string_attribute(str_arr){

	let arr = str_arr; // initialize final variable
	c = 0;
	while (c != arr.length-1) {
		 c = 0;
		  for (var i=0; i<arr.length-1; i++) {
			let cur = arr[i];
			  let next = arr[i+1];
			  let cur_value = cur.length;  // attribute that string array is sorted by
			  let next_value = next.length; // attribute that string array is sorted by
			  
			  if (cur_value < next_value) {
				  temp_cur = arr[i]
				  temp_next = arr[i+1]
				  arr[i+1] = temp_cur
				  arr[i] = temp_next
			  } else {
				  //this will be some value from 1 to arr.length-1
				  c = c + 1;
				  // c = arr.length-1 means that sorting is finished
			  }
		  }
	}
	
	return arr;
}

  // -------------------------------------------------
	
  async function create_tokenizer_dict(uniqueArray, tokenizer) {

	// ------------------------------------------

	// [0] Sort uniqueArray by an attribute (ie: length)
	var arr = await sort_an_array_by_string_attribute(uniqueArray);
	  
	// ------------------------------------------
  
	// [1] Make a dictionary where each key is a word in arr, and each value is index
	// Create a dictionary from one array
	tokenizer = Object.fromEntries(arr.map((word, index) => [word, index]));

	// ------------------------------------------

	// [2] Print the first 5 rows of the array, array_w_dictionary_rows
	// console.log('print_a_dictionary:');
	// print_a_dictionary(tokenizer);

	// ------------------------------------------

	 return tokenizer;
  }
	

	
  // -------------------------------------------------


async function tokenize_X(xs) {
	
	// ---------------------------
	
	// Concatenate the array contents into a long text string
	
	// Initialize with first array
	let final_str = xs.slice(0,1);
	// console.log('final_str 1st part:' + final_str);

	let xs_rest = xs.slice(1, xs.length); // 9 [Correct]
	// console.log('xs_rest:' + xs_rest.length);
	
	xs_rest.forEach(async function(val, ind) { 
		final_str = final_str.concat(val); // string
		// console.log('index: ' + ind + ', final_str type :' + typeof final_str); 
	});

	final_str = final_str.toString();
	// console.log('final_str accumulated type :' + typeof final_str); // final_str accumulated type :string [Correct]
	
	// ---------------------------
	
	// Replace any multiple spaces from accumulatted string with only a single space
	final_str = final_str.replace(/\s+/g, ' ');
	// console.log('final_str accumulated :' + final_str);
	
	// ---------------------------

	// Split the accumulatted string
	let final_arr = final_str.split(" ");
	console.log('final_arr.length :' + final_arr.length); // final_arr.length :2807

	// ---------------------------

	// Find unique words
	let uniqueArray = [...new Set(final_arr)];

	// Add the <OOV> token: if the word is not in the token list assign this value
	uniqueArray.push('<OOV>');
	console.log('uniqueArray.length :' + uniqueArray.length); // uniqueArray.length :1241

	// ---------------------------

	// Looks OK for now
	// Things to maybe concerned about: 0) there are commas after each word but maybe it is how it is printed to screen, 1) 3 words have two words joinned together by a comma
	tokenizer = await create_tokenizer_dict(uniqueArray, tokenizer);

	// ---------------------------

	// Apply tokenizer to sentences
	var xs_seq = await apply_tokenizer_to_sentences(xs, tokenizer);
	console.log("xs_seq: " + xs_seq);
	
	// ---------------------------
	
	return xs_seq;
}


async function apply_tokenizer_to_sentences(xs, tokenizer){

	// Apply tokenizer to sentences, to obtain sequences
	xs.forEach(async function(val, ind) {

		// Initialize
		if (ind == 0) { sen_arr_len = []; xs_seq = []; }

		// console.log("val original: " + val);
		
		// Replace any multiple spaces with only a single space
		val = val.replace(/\s+/g, ' ');
		
		// Convert string to string_array
		let sen_arr = val.split(" ");

		// Sort the string_array by word length
		sen_arr = await sort_an_array_by_string_attribute(sen_arr);
		// console.log("sen_arr: " + sen_arr);
		
		// Obtain the length of array for model training variable MAXLEN
		sen_arr_len.push(sen_arr.length);
		// console.log("sen_arr_len: " + sen_arr_len);

		sen_arr.forEach(async function(val1, ind1) {
			
			// Directly call the tokenizer, if the word does not exist put the number for '<OOV>' token
			let result = tokenizer[val1] ?? tokenizer['<OOV>'];
			
			// Replace all [val1 occurences] in the [string val], with the [token value result]
			regexp = new RegExp(`${val1}`, 'g');

			// Update the sentence string
			val = val.replace(regexp, result.toString());

			// Only print for first sentence
			// if (ind == 0){
			// 	console.log("val1: " + val1 + ", result: " + result);
			// 	console.log("matches_in_an_array: ", matches_in_an_array);
			// 	console.log("val transformed: " + val);
			// }
			
		});  // end of forEach

		// Convert string to string_array
		let seq_arr = val.split(" ");

		// Convert string_array to integer array
		let seq_arr_int = [];
		seq_arr.forEach(async function(val, ind) { seq_arr_int.push(Number(val)); });
		// console.log("seq_arr_int: " + seq_arr_int);
		
		// Save output sequence per row
		xs_seq.push(seq_arr_int);
		
	});  // end of forEach

	// Update the global variable of sen_arr, to initialize the model
	maxlen = await update_MAXLEN(sen_arr_len);
	
	return xs_seq;
}
	

  // -------------------------------------------------

  async function update_MAXLEN(sen_arr_len) {
	return tf.max(sen_arr_len);
  }
	
  // -------------------------------------------------

	
  async function predict_with_custom_model(y_assignment) {

	  const loadedModel = await tf.loadLayersModel('localstorage://my-model-1');
    
    var pred_vec = [['', '', ''],['row', 'prediction', 'probability']]; 

    const text_input = document.getElementById("input_text").value;

    text_input = text_input.split("\n")

    // ---------------------------
	  
    // Tokenize X
    // sequence_array
	  
    // ---------------------------

    var pred_vec = [];
		  
    sequence_array.forEach(async function(seq, ind) {
	  
	    // Call the custom_model 
	    const result = loadedModel.predict(seq) ;

	    // Get index of maximum softmax probability 
	    const index = result.as1D().argMax().dataSync()[0];
			
	    // Get maximum softmax probability 
	    const resultData = await result.data();    
	    const maxprob = resultData[index];

	    pred_vec.push([y_assignment[index], maxprob]);    
    
    });  // end of forEach

    // ---------------------------
	  
    // Save array to csv and put download csv file link on screen (NO AUTOMATIC DOWNLOADING)
    await put_results_in_CSVfile(pred_vec);

    // Create a table dynamically with the results
    await generateTable_dynamically(pred_vec);

    // ---------------------------	

  }  // end of predict_with_custom_model

	
  // -------------------------------------------------

	
  async function put_results_in_CSVfile(pred_vec) {
    
    // ---------------------
    // Puts array into csv format
    const blob = new Blob([pred_vec], { type: 'text/csv;charset=utf-8;' });

    // Create a url for the data object
    const url = URL.createObjectURL(blob);
    // ---------------------
    // OR
    // ---------------------
    // Puts array into csv format
    // let csvContent = "data:text/csv; charset=utf-8";
    // pred_vec.forEach(function(row_array) {
	// csvContent += row_array.join(",") + "\r\n";
    // });
    // OR
    // let csvContent = "data:text/csv;charset=utf-8," + rows.map(e => e.join(",")).join("\n");
	  
    // Create a url for the data object
    // var url = encodeURI(csvContent);
    // ---------------------

    const link = document.createElement("a");
    link.setAttribute("href", url);

    const filename = 'data.csv';
    link.setAttribute("download", filename);
	  
    link.style.display = 'block';

    outp.innerHTML += 'Download CSV here: ' + url + "<br/>";
    // document.body.appendChild(link);

    // Automatic download of csv
    // link.click();
    // document.body.removeChild(link);
	  
  }

	
  // -------------------------------------------------

	
  async function generateTable_dynamically(pred_vec) {

  	const tbl = document.createElement("table");
  
  	const tblBody = document.createElement("tbody");
  
  	// Create row cells dynamically
  	for (let i=0; i < pred_vec.length; i++){
  		// create a table row
  		const row = document.createElement("tr");
  
  		for (let j=0; j < pred_vec[0].length; j++){
  			const cell = document.createElement("td");
  			const cellText = document.createTextNode(`${pred_vec[i][j]}`);
  			cell.appendChild(cellText);
  			row.appendChild(cell);
  		}
  
  		// add a row to the end of the table
  		tblBody.appendChild(row);
  	}
  	tbl.appendChild(tblBody);
  
  	document.body.appendChild(tbl);

  }  

  // -------------------------------------------------

	
</script>
</body>
</html>
