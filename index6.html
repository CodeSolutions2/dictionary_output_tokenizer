<!DOCTYPE html>
<html>
<head></head>
<body>

<h1>Tokenizer webapp</h1>
In the field of text classification, making a tokenizer is the first thing that one must do to a body of text before modeling. Tokenizers are plentiful and often use the regex library to parse text.
<ol type="A">
	<li>Enter text.</li>
	<li>Display tokenized text.</li>
</ol>
<br><br>

<!-- ---------------------------------------- -->
<!-- View two split window -->
<div align="left">
<table style='text-align: left; width: 500px; display:block'>
<tr>

<th id="table_leftside_input">

<h3>[Step 0] Comparing my tokenizer to other npm/jsdelivr tokenizers.</h3>

<ol type="A">
	<li>dictionary_output_tokenizer: easy to use, fast processing because replacement of words with tokens are performed with regex, currently one rule (word tokenization)</li>
	<li>npm_tokenizers: Multiple rules for tokenization, tokenization output for model usage (ie: BERT, GPT, etc)</li>
</ol>

<h3 id="modify_tokenizer_h3" style="display:none;">Modify tokenizer: delete tokens. If there are no tokens to modify, run dictionary_output_tokenizer first.</h3>
<p id="modify_tokenizer_p" style="display:none;">Only for dictionary_output_tokenizer: List the token keys to delete, for example for the tokenizer {"dictionary_output_tokenizer":0,"npm_tokenizers":1,"tokenization":2,"tokenizers,":3} input "dictionary_output_tokenizer, npm_tokenizers" to remove the first two tokens.</p>
<textarea id="modify_tokenizer" wrap="soft" rows="10" cols="100" placeholder="input text" style="display:none; text-align: left; width: 600px; height: 200px;"></textarea>
  
<!-- Drop down menu: put in body -->
<label for="select_dropdown_option_label">Select a drop down option:</label>
<select name="dropdown_options" id="dropdown_options" style="display:block">
  <option value="---">Select an option</option>
  <option value="npm_tokenizers">npm_tokenizers</option>
  <option value="dictionary_output_tokenizer_v1">dictionary_output_tokenizer_v1</option>
  <option value="dictionary_output_tokenizer_deleteTokens_v1">dictionary_output_tokenizer_deleteTokens_v1</option>
  <option value="dictionary_output_tokenizer_encode_v1">dictionary_output_tokenizer_encode_v1</option>
  <option value="dictionary_output_tokenizer_decode_v1">dictionary_output_tokenizer_decode_v1</option>

<br><br>
  
<h3>[Step 1] Input text.</h3>
<textarea id="input_text"  wrap="soft" rows="10" cols="100" placeholder="input text" style="display:block; text-align: left; width: 600px; height: 400px;"></textarea>

<br><br>
  
<h3>[Step 2] Tokenize text.</h3>
<button id="run_selection" onclick="run_selection()" style="display:block">Step 2: run selection</button>

<br><br>

<progress id="progress_bar" max="100" value="0" style="display:none">0%</progress>

<!-- ---------------------------------------- -->

<th id="table_rightside_output">
<h3>[Step 1] View Results.</h3>
<div id="data_display" style="display:block; text-align: left; width: 600px; height: 600px">
<br>
<div id="notification"></div>
<br>
<div id="error"></div>
</th>
	
</tr>
</table>
</div>  
<!-- ---------------------------------------- -->



<!-- ---------------------------------------- -->
<!-- CSS -->
<style>
div { padding: 10px; display:block; font-family:courier; font-size:15px; }
div#notification { position: relative; color: #3236a8; }
div#error { position: relative; color: #bd1f17; }

table {vertical-align: top; border-collapse: collapse; position: relative; z-index: 0; border: 0px solid black;}

tr {vertical-align: top; border: 0px solid black; padding: 30px 30px; }

th, td {vertical-align: top; border: 0px solid black; padding: 10px; }
th#table_leftside_input {width: 100%; }
th#table_rightside_output {width: 100%; }

div#data_display {position: absolute; vertical-align: top; top: 200; z-index: 200; width: 500px; word-break: break-all; word-wrap: break-word; }
  
</style>

<!-- ---------------------------------------- -->

<!-- gpt-tokenizer: WORKS! -->
<!-- <script src="https://cdn.jsdelivr.net/npm/gpt-tokenizer@2.5.0/dist/cl100k_base.min.js"></script> -->

<!-- gpt-tokenizer: WORKS! -->
<script src="https://unpkg.com/gpt-tokenizer@2.5.0/dist/cl100k_base.js"></script>
  
	
<!-- ---------------------------------------- -->

<script type="module">
<!-- import { create_tokenizer, delete_key_value, encode, decode } from "./dist/tokenizer_1.js";
module2.create_tokenizer = create_tokenizer;
module3.delete_key_value = delete_key_value;
module4.encode = encode;
module5.decode = decode;
</script>

<!-- ---------------------------------------- -->

<script>


// -----------------------------------------------
	
window.addEventListener('beforeunload', function() {
	window.location.href = window.location.href + '?nocache=' + new Date().getTime();
	document.getElementById("dropdown_options").selectedIndex = 0;
});

// ----------------------------------------------------

// Put in <script>
async function processEvent_dropdown_options(event) {

	if (document.getElementById("dropdown_options").selectedIndex == 3) {
    
		document.getElementById("modify_tokenizer_h3").style.display = 'block';
		document.getElementById("modify_tokenizer_p").style.display = 'block';
		document.getElementById("modify_tokenizer").style.display = 'block';

	} else {
		document.getElementById("modify_tokenizer_h3").style.display = 'none';
		document.getElementById("modify_tokenizer_p").style.display = 'none';
		document.getElementById("modify_tokenizer").style.display = 'none';
	}
	
}

document.getElementById("dropdown_options").addEventListener("change", processEvent_dropdown_options, false);

// ----------------------------------------------------

const module2 = {};
const module3 = {};
const module4 = {};
const module5 = {};
var xs;
var tokenizer_obj;
	
// ----------------------------------------------------

async function run_selection() {
	
	var input_text = document.getElementById('input_text').value;
	// console.log("input_text: ", input_text);
	
	document.getElementById("progress_bar").style.display = "block";
	document.getElementById("progress_bar").value = (1/3)*100;
	
	// -------------------
  
	// Put in a function in <script>
	var dropdown_option_type = document.getElementById("dropdown_options").value;
	
	// -------------------
	
	console.time('algo');

	// -------------------
	
	if (dropdown_option_type == 'npm_tokenizers') {

		// https://www.jsdelivr.com/package/npm/gpt-tokenizer
		const { encode, decode } = GPTTokenizer_cl100k_base;
		tokenizer_obj = await encode(input_text);
		// console.log("tokenizer_obj: ", tokenizer_obj);

		// algo: 68ms - timer ended

  		
	} else if (dropdown_option_type == 'dictionary_output_tokenizer_v1') {

		[xs, tokenizer_obj] = await module2.create_tokenizer(input_text);
		// OR
		// [xs, tokenizer_obj] = await create_tokenizer(input_text);

		// console.log("xs: ", xs);
		// console.log("tokenizer_obj: ", tokenizer_obj);
		
		// Highlight original text
		// var textarea = document.getElementById("input_text");
		// textarea.setSelectionRange(5, 9); // Highlight text from index 5 to 9
		// textarea.focus();
		// for (let i=0; i < tokenizer_obj.length; i++) {
		// }

		
	} else if (dropdown_option_type == 'dictionary_output_tokenizer_deleteTokens_v1') {
		
		var key_names_str = document.getElementById('modify_tokenizer').value;
		tokenizer_obj = await module3.delete_key_value(tokenizer_obj, key_names_str);
		document.getElementById('data_display').innerHTML = JSON.stringify(tokenizer_obj);

		
	} else if (dropdown_option_type == 'dictionary_output_tokenizer_encode_v1') {

		var xs_encoded = await module4.encode(xs, tokenizer_obj);
		document.getElementById('data_display').innerHTML = JSON.stringify(xs_encoded);

		
	} else if (dropdown_option_type == 'dictionary_output_tokenizer_decode_v1') {

		var xs_decoded = await module5.decode(xs_tokens, tokenizer_obj);
		document.getElementById('data_display').innerHTML = JSON.stringify(xs_decoded);

		
	} else {
		
		document.getElementById('notification').innerHTML = "Please select a drop down option type."; 
	}

	console.log("xs: ", xs);

	// -------------------
	
	console.timeEnd('algo');
	
	// -------------------
	
	document.getElementById("progress_bar").value = (2/3)*100;
	document.getElementById('data_display').innerHTML = JSON.stringify(tokenizer_obj);
	document.getElementById("progress_bar").value = (3/3)*100;
	document.getElementById("progress_bar").style.display = "none";

}
  
// ----------------------------------------------------



async function create_tokenizer(input_text) {
	
	var csvDataset = await convert_text_to_csv(input_text);
	console.log("csvDataset: ", csvDataset);
	
	var xs = await preprocess_xs(csvDataset);
	console.log("xs: ", xs);
	
	var tokenizer = await tokenize_X(xs);
	console.log("tokenizer: ", tokenizer);
	
	return [xs, tokenizer];
}

// -------------------------------------------------

async function delete_key_value(tokenizer_obj, key_names_str) {

	key_names_str = key_names_str.replace(/\s+/g, " ");
	key_names_str = key_names_str.replace(/,\s/g, ",");
	key_names_str = key_names_str.replace(/\s,/g, ",");
	// console.log("key_names_str: ", key_names_str);

	var key_names_arr = key_names_str.split(",");
	// console.log("key_names_arr: ", key_names_arr);

	for (let i=0; i<key_names_arr.length; i++) {
		delete tokenizer_obj[key_names_arr.at(i)];
	}

	// Re-name the values from 0 to n
	var tokenizer_obj_modified = {};
	Object.keys(tokenizer_obj).map((val, ind) => { tokenizer_obj_modified[val] = ind; })
	// console.log("tokenizer_obj_modified: ", tokenizer_obj_modified)
	
	return tokenizer_obj_modified;
}

// -------------------------------------------------

async function encode(xs, tokenizer_obj) {
    
    var xs_encoded = [];
    
    for (let i=0; i<xs.length; i++) {

        var xs_final_str = xs.at(i).replace(/\s+/g, ' '); // make repeated space only one space
        xs_final_str = xs_final_str.replace(/^\s+|\s+$/g, ''); // remove spaces at the beginning and end
        // console.log("xs_final_str: ", xs_final_str)

        // Convert string to array
        var xs_final_arr = xs_final_str.split(' ');
        
        // Way 1 - work with string array and loop over every entry : use map
        var xs_tokens = xs_final_arr.map((val, ind) => { 
            return tokenizer_obj[val] ?? tokenizer_obj['<OOV>']; 
        });
        // console.log("xs_tokens: ", xs_tokens)
        
        xs_encoded.push(xs_tokens)
    }
    return xs_encoded;
}

// -------------------------------------------------

async function decode(xs_tokens, tokenizer_obj) {
    
    var xs_decoded = [];

    // Invert the keys with the values for the tokenizer
    var tokenizer_obj_inverted = {};
    Object.entries(tokenizer_obj).map(([key, value]) => { tokenizer_obj_inverted[value] = key; });
    
    for (let i=0; i<xs_tokens.length; i++) {
        
        // Way 1 - work with string array and loop over every entry : use map
        var xs = xs_tokens.at(i).map((val, ind) => { return tokenizer_obj_inverted[val]; });
        // console.log("xs: ", xs)
        
        var xs_final = xs.join(' ');
        // console.log("xs_final: ", xs_final)
            
        xs_decoded.push(xs_final)
    }
    return xs_decoded;
}

// -------------------------------------------------

async function convert_text_to_csv(input_text) {

	var sentences = input_text.split(".");
	// console.log("sentences: ", sentences);
  
	var csvDataset = [];
    for (var i=0; i<sentences.length; i++) {
	    var tf_obj = {xs: sentences.at(i), ys: "none"};
	    csvDataset.push(tf_obj);
    }
	
	return csvDataset;
}

// -------------------------------------------------

async function preprocess_xs(csvDataset) {

  //const tensors = await csvDataset.toArray();

  var X = '';
  var xs = [];

  csvDataset.forEach(async function(rowdata, index) {

	console.log("rowdata.xs: ", rowdata.xs);
	  
	 // ---------------------------
	 // Clean text procedure
	 // ---------------------------
	 // Make text lower case, Remove characters between parenthesis, Remove text that are 1 or 2 characters long, Remove undesireable characters
	 X = Object.values(rowdata.xs).toString().toLowerCase().replace(/\((.*?)\)/g, '').replace(/\{(.*?)\}/g, '').replace(/\[(.*?)\]/g, '').replace(/[/\/\n]/g, ' ').replace(/[\.\€\$\£\%\d,\[\]\(\)\{\}\!-><\n]/g, '').replace(/(?<=\s)[A-Za-z]{1,2}(?=\s)/g, '');  
	
	  // ---------------------------
	  // Verify that data is correct, and only keep correct X and Y data
	  // if (X.length-1 > 10){ xs.push(X); }
	  xs.push(X);

  });  // end of forEach


	console.log("xs: ", xs);

  return xs;
}

// -------------------------------------------------




	
// -------------------------------------------------
// SUBFUNCTIONS
// -------------------------------------------------
async function tokenize_X(xs) {
	
	// ---------------------------
	
	// Concatenate the array contents into a long text string
	
	// Initialize with first array
	let final_str = xs.slice(0,1);
	let xs_rest = xs.slice(1, xs.length);
	
	xs_rest.forEach(async function(val, ind) { 
		final_str = final_str.concat(val); // string
	});

	final_str = final_str.toString();
	
	// ---------------------------
	
	// Replace any multiple spaces from accumulatted string with only a single space
	final_str = final_str.replace(/\s+/g, ' ');
	
	// ---------------------------

	// Split the accumulatted string
	let final_arr = final_str.split(" ");

	// ---------------------------

	// Find unique words
	let uniqueArray = [...new Set(final_arr)];

	// Add the <OOV> token: if the word is not in the token list assign this value
	uniqueArray.push('<OOV>');
	
	// ---------------------------

	return await create_tokenizer_dict(uniqueArray);
}

// -------------------------------------------------

async function create_tokenizer_dict(uniqueArray) {

	console.log("uniqueArray: ", uniqueArray);
	
	// [0] Sort uniqueArray by an attribute (ie: length)
	let arr = uniqueArray;
	var c = 0;
	while (c != arr.length-1) {
		 c = 0;
		  for (var i=0; i<arr.length-1; i++) {
			let cur = arr[i];
			  let next = arr[i+1];
			  let cur_value = cur.length;
			  let next_value = next.length;
			  
			  if (cur_value < next_value) {
				  let temp_cur = arr[i];
				  let temp_next = arr[i+1]; // want the old value of arr[i+1]
				  arr[i+1] = temp_cur;
				  arr[i] = temp_next;
			  } else {
				  //this will be some value from 1 to arr.length-1
				  c = c + 1;
				  // c = arr.length-1 means that sorting is finished
			  }
		  }
	}
	console.log("arr: ", arr);
  
	// [1] Make a dictionary where each key is a word in arr, and each value is index
	// Create a dictionary from one array
	let tokenizer_obj = Object.fromEntries(arr.map((word, index) => { 
		return [word.replace(/[,.]/g, '').toString(), index]; 
	} ));

	return tokenizer_obj;
}

// ----------------------------------------------------


	
</script>
</body>
</html>
